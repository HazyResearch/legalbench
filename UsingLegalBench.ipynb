{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a basic illustration of how to use different parts of LegalBench. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task organization\n",
    "\n",
    "`tasks.py` provides data structures which organize all LegalBench tasks. For instance, `TASKS` lists all LegalBench tasks, and `ISSUE_TASKS` lists all tasks in the issue-spotting reasoning category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 ['abercrombie', 'canada_tax_court_outcomes', 'citation_prediction_classification', 'citation_prediction_open', 'consumer_contracts_qa', 'contract_nli_confidentiality_of_agreement', 'contract_nli_explicit_identification', 'contract_nli_inclusion_of_verbally_conveyed_information', 'contract_nli_limited_use', 'contract_nli_no_licensing']\n",
      "\n",
      "17 ['corporate_lobbying', 'learned_hands_benefits', 'learned_hands_business', 'learned_hands_consumer', 'learned_hands_courts', 'learned_hands_crime', 'learned_hands_divorce', 'learned_hands_domestic_violence', 'learned_hands_education', 'learned_hands_employment', 'learned_hands_estates', 'learned_hands_family', 'learned_hands_health', 'learned_hands_housing', 'learned_hands_immigration', 'learned_hands_torts', 'learned_hands_traffic']\n"
     ]
    }
   ],
   "source": [
    "from tasks import TASKS, ISSUE_TASKS\n",
    "\n",
    "print(len(TASKS), TASKS[:10])\n",
    "print()\n",
    "print(len(ISSUE_TASKS), ISSUE_TASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading task data\n",
    "\n",
    "LegalBench can be downloaded from Huggingface: https://huggingface.co/datasets/nguha/legalbench. Each LegalBench dataset comes with `train` and `test` split.\n",
    "\n",
    "- The `train` split is small (usually fewer than 10 samples). Following the [RAFT](https://raft.elicit.org/) benchmark, it's intended to provide labaled samples that can be used as few-shot demonstrations for prompts.\n",
    "- The `test` split is larger, and contains samples to evaluate an LLM on. \n",
    "\n",
    "Documentation for each task can be found on the Github repository, under the task-specific folder. For instance, the documentation for the `abercrombie` task can be found at <https://github.com/HazyResearch/legalbench/tree/main/tasks/abercrombie>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Found cached dataset legalbench (/Users/neelguha/.cache/huggingface/datasets/nguha___legalbench/abercrombie/1.0.0/2a34f0ce703d569b2f615c8acf935c72662a7bf3034c4dd60d8aaf1e5a6168c6)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 535.09it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generic</td>\n",
       "      <td>0</td>\n",
       "      <td>The mark \"Ivory\" for a product made of elephan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>descriptive</td>\n",
       "      <td>1</td>\n",
       "      <td>The mark \"Tasty\" for bread.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suggestive</td>\n",
       "      <td>2</td>\n",
       "      <td>The mark \"Caress\" for body soap.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arbitrary</td>\n",
       "      <td>3</td>\n",
       "      <td>The mark \"Virgin\" for wireless communications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fanciful</td>\n",
       "      <td>4</td>\n",
       "      <td>The mark \"Aswelly\" for a taxi service.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        answer index                                               text\n",
       "0      generic     0  The mark \"Ivory\" for a product made of elephan...\n",
       "1  descriptive     1                        The mark \"Tasty\" for bread.\n",
       "2   suggestive     2                   The mark \"Caress\" for body soap.\n",
       "3    arbitrary     3     The mark \"Virgin\" for wireless communications.\n",
       "4     fanciful     4             The mark \"Aswelly\" for a taxi service."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"nguha/legalbench\", \"abercrombie\")\n",
    "dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and applying prompts\n",
    "\n",
    "Each task folder also stores prompt templates which can be used with different models. In LegalBench, prompt templates are represented as text files, in which \"{{col_name}}\" denote place holders for column names.\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A mark is generic if it is the common name for the product. A mark is descriptive if it describes a purpose, nature, or attribute of the product. A mark is suggestive if it suggests or implies a quality or characteristic of the product. A mark is arbitrary if it is a real English word that has no relation to the product. A mark is fanciful if it is an invented word.\n",
      "\n",
      "Q: The mark \"Ivory\" for a product made of elephant tusks. What is the type of mark?\n",
      "A: generic\n",
      "\n",
      "Q: The mark \"Tasty\" for bread. What is the type of mark?\n",
      "A: descriptive\n",
      "\n",
      "Q: The mark \"Caress\" for body soap. What is the type of mark?\n",
      "A: suggestive\n",
      "\n",
      "Q: The mark \"Virgin\" for wireless communications. What is the type of mark?\n",
      "A: arbitrary\n",
      "\n",
      "Q: The mark \"Aswelly\" for a taxi service. What is the type of mark?\n",
      "A: fanciful\n",
      "\n",
      "Q: {{text}} What is the type of mark?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "# Load base prompt\n",
    "with open(f\"tasks/abercrombie/base_prompt.txt\") as in_file:\n",
    "    prompt_template = in_file.read()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script `utils.py` provides a simple function for generating prompts for a dataset given a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A mark is generic if it is the common name for the product. A mark is descriptive if it describes a purpose, nature, or attribute of the product. A mark is suggestive if it suggests or implies a quality or characteristic of the product. A mark is arbitrary if it is a real English word that has no relation to the product. A mark is fanciful if it is an invented word.\n",
      "\n",
      "Q: The mark \"Ivory\" for a product made of elephant tusks. What is the type of mark?\n",
      "A: generic\n",
      "\n",
      "Q: The mark \"Tasty\" for bread. What is the type of mark?\n",
      "A: descriptive\n",
      "\n",
      "Q: The mark \"Caress\" for body soap. What is the type of mark?\n",
      "A: suggestive\n",
      "\n",
      "Q: The mark \"Virgin\" for wireless communications. What is the type of mark?\n",
      "A: arbitrary\n",
      "\n",
      "Q: The mark \"Aswelly\" for a taxi service. What is the type of mark?\n",
      "A: fanciful\n",
      "\n",
      "Q: The mark “Salt” for packages of sodium chloride. What is the type of mark?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "from utils import generate_prompts\n",
    "\n",
    "test_df = dataset[\"test\"].to_pandas()\n",
    "prompts = generate_prompts(prompt_template=prompt_template, data_df=test_df)\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The majority of LegalBench tasks are evaluated using balanced-accuracy. A handful of tasks which involve extraction or multilabel classification are evaluated using F1. To simplify evaluation, we provide an evaluation which which scores performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25263157894736843"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Generate random predictions for abercrombie\n",
    "classes = [\"generic\", \"descriptive\", \"suggestive\", \"arbitrary\", \"fanciful\"]\n",
    "generations = np.random.choice(classes, len(test_df))\n",
    "\n",
    "evaluate(\"abercrombie\", generations, test_df[\"answer\"].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
