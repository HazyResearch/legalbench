{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a basic illustration of how to use different parts of LegalBench. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/neelguha/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import datasets\n",
    "\n",
    "from tasks import TASKS, ISSUE_TASKS\n",
    "from utils import generate_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supress progress bars which appear every time a task is downloaded\n",
    "datasets.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task organization\n",
    "\n",
    "`tasks.py` provides data structures which organize all LegalBench tasks. For instance, `TASKS` lists all LegalBench tasks, and `ISSUE_TASKS` lists all tasks in the issue-spotting reasoning category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162 ['abercrombie', 'canada_tax_court_outcomes', 'citation_prediction_classification', 'citation_prediction_open', 'consumer_contracts_qa', 'contract_nli_confidentiality_of_agreement', 'contract_nli_explicit_identification', 'contract_nli_inclusion_of_verbally_conveyed_information', 'contract_nli_limited_use', 'contract_nli_no_licensing']\n",
      "\n",
      "17 ['corporate_lobbying', 'learned_hands_benefits', 'learned_hands_business', 'learned_hands_consumer', 'learned_hands_courts', 'learned_hands_crime', 'learned_hands_divorce', 'learned_hands_domestic_violence', 'learned_hands_education', 'learned_hands_employment', 'learned_hands_estates', 'learned_hands_family', 'learned_hands_health', 'learned_hands_housing', 'learned_hands_immigration', 'learned_hands_torts', 'learned_hands_traffic']\n"
     ]
    }
   ],
   "source": [
    "print(len(TASKS), TASKS[:10])\n",
    "print()\n",
    "print(len(ISSUE_TASKS), ISSUE_TASKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading task data\n",
    "\n",
    "LegalBench can be downloaded from Huggingface: https://huggingface.co/datasets/nguha/legalbench. Each LegalBench dataset comes with `train` and `test` split.\n",
    "\n",
    "- The `train` split is small (usually fewer than 10 samples). Following the [RAFT](https://raft.elicit.org/) benchmark, it's intended to provide labaled samples that can be used as few-shot demonstrations for prompts.\n",
    "- The `test` split is larger, and contains samples to evaluate an LLM on. \n",
    "\n",
    "Documentation for each task can be found on the Github repository, under the task-specific folder. For instance, the documentation for the `abercrombie` task can be found at <https://github.com/HazyResearch/legalbench/tree/main/tasks/abercrombie>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>generic</td>\n",
       "      <td>0</td>\n",
       "      <td>The mark \"Ivory\" for a product made of elephan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>descriptive</td>\n",
       "      <td>1</td>\n",
       "      <td>The mark \"Tasty\" for bread.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>suggestive</td>\n",
       "      <td>2</td>\n",
       "      <td>The mark \"Caress\" for body soap.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>arbitrary</td>\n",
       "      <td>3</td>\n",
       "      <td>The mark \"Virgin\" for wireless communications.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fanciful</td>\n",
       "      <td>4</td>\n",
       "      <td>The mark \"Aswelly\" for a taxi service.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        answer index                                               text\n",
       "0      generic     0  The mark \"Ivory\" for a product made of elephan...\n",
       "1  descriptive     1                        The mark \"Tasty\" for bread.\n",
       "2   suggestive     2                   The mark \"Caress\" for body soap.\n",
       "3    arbitrary     3     The mark \"Virgin\" for wireless communications.\n",
       "4     fanciful     4             The mark \"Aswelly\" for a taxi service."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"nguha/legalbench\", \"abercrombie\")\n",
    "dataset[\"train\"].to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and applying prompts\n",
    "\n",
    "Each task folder also stores prompt templates which can be used with different models. In LegalBench, prompt templates are represented as text files, in which \"{{col_name}}\" denote place holders for column names.\n",
    "\n",
    "For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A mark is generic if it is the common name for the product. A mark is descriptive if it describes a purpose, nature, or attribute of the product. A mark is suggestive if it suggests or implies a quality or characteristic of the product. A mark is arbitrary if it is a real English word that has no relation to the product. A mark is fanciful if it is an invented word.\n",
      "\n",
      "Q: The mark \"Ivory\" for a product made of elephant tusks. What is the type of mark?\n",
      "A: generic\n",
      "\n",
      "Q: The mark \"Tasty\" for bread. What is the type of mark?\n",
      "A: descriptive\n",
      "\n",
      "Q: The mark \"Caress\" for body soap. What is the type of mark?\n",
      "A: suggestive\n",
      "\n",
      "Q: The mark \"Virgin\" for wireless communications. What is the type of mark?\n",
      "A: arbitrary\n",
      "\n",
      "Q: The mark \"Aswelly\" for a taxi service. What is the type of mark?\n",
      "A: fanciful\n",
      "\n",
      "Q: {{text}} What is the type of mark?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "# Load base prompt\n",
    "with open(f\"tasks/abercrombie/base_prompt.txt\") as in_file:\n",
    "    prompt_template = in_file.read()\n",
    "print(prompt_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script `utils.py` provides a simple function for generating prompts for a dataset given a template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A mark is generic if it is the common name for the product. A mark is descriptive if it describes a purpose, nature, or attribute of the product. A mark is suggestive if it suggests or implies a quality or characteristic of the product. A mark is arbitrary if it is a real English word that has no relation to the product. A mark is fanciful if it is an invented word.\n",
      "\n",
      "Q: The mark \"Ivory\" for a product made of elephant tusks. What is the type of mark?\n",
      "A: generic\n",
      "\n",
      "Q: The mark \"Tasty\" for bread. What is the type of mark?\n",
      "A: descriptive\n",
      "\n",
      "Q: The mark \"Caress\" for body soap. What is the type of mark?\n",
      "A: suggestive\n",
      "\n",
      "Q: The mark \"Virgin\" for wireless communications. What is the type of mark?\n",
      "A: arbitrary\n",
      "\n",
      "Q: The mark \"Aswelly\" for a taxi service. What is the type of mark?\n",
      "A: fanciful\n",
      "\n",
      "Q: The mark “Salt” for packages of sodium chloride. What is the type of mark?\n",
      "A:\n"
     ]
    }
   ],
   "source": [
    "test_df = dataset[\"test\"].to_pandas()\n",
    "prompts = generate_prompts(prompt_template=prompt_template, data_df=test_df)\n",
    "print(prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "The majority of LegalBench tasks are evaluated using balanced-accuracy. A handful of tasks which involve extraction or multilabel classification are evaluated using F1. To simplify evaluation, we provide an evaluation which which scores performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23157894736842105"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluation import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Generate random predictions for abercrombie\n",
    "classes = [\"generic\", \"descriptive\", \"suggestive\", \"arbitrary\", \"fanciful\"]\n",
    "generations = np.random.choice(classes, len(test_df))\n",
    "\n",
    "evaluate(\"abercrombie\", generations, test_df[\"answer\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting tasks by license\n",
    "\n",
    "LegalBench tasks are covered under different licenses. The following code allows you to filter out tasks by license type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 162/162 [03:00<00:00,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks with target license: ['abercrombie', 'citation_prediction_classification', 'citation_prediction_open', 'contract_nli_confidentiality_of_agreement', 'contract_nli_explicit_identification', 'contract_nli_inclusion_of_verbally_conveyed_information', 'contract_nli_limited_use', 'contract_nli_no_licensing', 'contract_nli_notice_on_compelled_disclosure', 'contract_nli_permissible_acquirement_of_similar_information', 'contract_nli_permissible_copy', 'contract_nli_permissible_development_of_similar_information', 'contract_nli_permissible_post-agreement_possession', 'contract_nli_return_of_confidential_information', 'contract_nli_sharing_with_employees', 'contract_nli_sharing_with_third-parties', 'contract_nli_survival_of_obligations', 'contract_qa', 'corporate_lobbying', 'cuad_affiliate_license-licensee', 'cuad_affiliate_license-licensor', 'cuad_anti-assignment', 'cuad_audit_rights', 'cuad_cap_on_liability', 'cuad_change_of_control', 'cuad_competitive_restriction_exception', 'cuad_covenant_not_to_sue', 'cuad_effective_date', 'cuad_exclusivity', 'cuad_expiration_date', 'cuad_governing_law', 'cuad_insurance', 'cuad_ip_ownership_assignment', 'cuad_irrevocable_or_perpetual_license', 'cuad_joint_ip_ownership', 'cuad_license_grant', 'cuad_liquidated_damages', 'cuad_minimum_commitment', 'cuad_most_favored_nation', 'cuad_no-solicit_of_customers', 'cuad_no-solicit_of_employees', 'cuad_non-compete', 'cuad_non-disparagement', 'cuad_non-transferable_license', 'cuad_notice_period_to_terminate_renewal', 'cuad_post-termination_services', 'cuad_price_restrictions', 'cuad_renewal_term', 'cuad_revenue-profit_sharing', 'cuad_rofr-rofo-rofn', 'cuad_source_code_escrow', 'cuad_termination_for_convenience', 'cuad_third_party_beneficiary', 'cuad_uncapped_liability', 'cuad_unlimited-all-you-can-eat-license', 'cuad_volume_restriction', 'cuad_warranty_duration', 'diversity_1', 'diversity_2', 'diversity_3', 'diversity_4', 'diversity_5', 'diversity_6', 'function_of_decision_section', 'hearsay', 'insurance_policy_interpretation', 'jcrew_blocker', 'international_citizenship_questions', 'legal_reasoning_causality', 'maud_ability_to_consummate_concept_is_subject_to_mae_carveouts', 'maud_financial_point_of_view_is_the_sole_consideration', 'maud_accuracy_of_fundamental_target_rws_bringdown_standard', 'maud_accuracy_of_target_general_rw_bringdown_timing_answer', 'maud_accuracy_of_target_capitalization_rw_(outstanding_shares)_bringdown_standard_answer', 'maud_additional_matching_rights_period_for_modifications_(cor)', 'maud_application_of_buyer_consent_requirement_(negative_interim_covenant)', 'maud_buyer_consent_requirement_(ordinary_course)', 'maud_change_in_law__subject_to_disproportionate_impact_modifier', 'maud_changes_in_gaap_or_other_accounting_principles__subject_to_disproportionate_impact_modifier', 'maud_cor_permitted_in_response_to_intervening_event', 'maud_cor_permitted_with_board_fiduciary_determination_only', 'maud_cor_standard_(intervening_event)', 'maud_cor_standard_(superior_offer)', 'maud_definition_contains_knowledge_requirement_-_answer', 'maud_definition_includes_asset_deals', 'maud_definition_includes_stock_deals', 'maud_fiduciary_exception__board_determination_standard', 'maud_fiduciary_exception_board_determination_trigger_(no_shop)', 'maud_fls_(mae)_standard', 'maud_general_economic_and_financial_conditions_subject_to_disproportionate_impact_modifier', 'maud_includes_consistent_with_past_practice', 'maud_initial_matching_rights_period_(cor)', 'maud_initial_matching_rights_period_(ftr)', 'maud_intervening_event_-_required_to_occur_after_signing_-_answer', 'maud_knowledge_definition', 'maud_liability_standard_for_no-shop_breach_by_target_non-do_representatives', 'maud_ordinary_course_efforts_standard', 'maud_pandemic_or_other_public_health_event__subject_to_disproportionate_impact_modifier', 'maud_pandemic_or_other_public_health_event_specific_reference_to_pandemic-related_governmental_responses_or_measures', 'maud_relational_language_(mae)_applies_to', 'maud_specific_performance', 'maud_tail_period_length', 'maud_type_of_consideration', 'oral_argument_question_purpose', 'overruling', 'personal_jurisdiction', 'proa', 'rule_qa', 'scalr', 'ssla_company_defendants', 'ssla_individual_defendants', 'ssla_plaintiff', 'successor_liability', 'supply_chain_disclosure_best_practice_accountability', 'supply_chain_disclosure_best_practice_audits', 'supply_chain_disclosure_best_practice_certification', 'supply_chain_disclosure_best_practice_training', 'supply_chain_disclosure_best_practice_verification', 'supply_chain_disclosure_disclosed_accountability', 'supply_chain_disclosure_disclosed_audits', 'supply_chain_disclosure_disclosed_certification', 'supply_chain_disclosure_disclosed_training', 'supply_chain_disclosure_disclosed_verification', 'telemarketing_sales_rule', 'ucc_v_common_law', 'unfair_tos']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "target_license = \"CC BY 4.0\"\n",
    "tasks_with_target_license = []\n",
    "for task in tqdm(TASKS):\n",
    "    dataset = datasets.load_dataset(\"nguha/legalbench\", task, split=\"train\")\n",
    "    if dataset.info.license == target_license:\n",
    "        tasks_with_target_license.append(task)\n",
    "print(\"Tasks with target license:\", tasks_with_target_license)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
